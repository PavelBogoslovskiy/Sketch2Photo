{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cv_project_bicycle.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14kzTHzc1dybhYcZmM0Th4OTl-SmlZWfm","authorship_tag":"ABX9TyNRpf7wBsFRh+9ShbFme6pD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vbPwx_zL6hVw","executionInfo":{"status":"ok","timestamp":1643789617687,"user_tz":-180,"elapsed":6526,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["import os\n","import sys\n","import glob\n","import math\n","import time\n","import random\n","import itertools\n","import datetime\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.models import resnet18\n","from torchvision.utils import save_image\n","import torchvision.transforms as transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Data preprocessing for the dataloader"],"metadata":{"id":"9PyiTGmfGjaM"}},{"cell_type":"code","metadata":{"id":"jqyTFL4Xks9A","executionInfo":{"status":"ok","timestamp":1643789692505,"user_tz":-180,"elapsed":362,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class ImageDataset(Dataset):\n","    def __init__(self, root, input_shape, mode=\"train1\"):\n","        self.transform = transforms.Compose(\n","            [\n","                transforms.Resize((128,128), Image.BICUBIC), #input_shape[-2:], Image.BICUBIC),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n","            ]\n","        )\n","\n","        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n","        self.mode = mode\n","\n","    def __getitem__(self, index):\n","\n","        img = Image.open(self.files[index % len(self.files)])\n","        w, h = img.size\n","        img_A = img.crop((0, 0, w / 2, h))\n","        img_B = img.crop((w / 2, 0, w, h))\n","\n","        if np.random.random() < 0.5 and self.mode == \"train1\":\n","            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n","            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n","\n","        img_A = self.transform(img_A)\n","        img_B = self.transform(img_B)\n","\n","        return {\"A\": img_A, \"B\": img_B}\n","\n","    def __len__(self):\n","        return len(self.files)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Populates the weights with values from a normal distribution"],"metadata":{"id":"MdV-0oTlGyIU"}},{"cell_type":"code","metadata":{"id":"5An6-oef8AqY","executionInfo":{"status":"ok","timestamp":1643789694724,"user_tz":-180,"elapsed":256,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SR5saSLM82_s"},"source":["Parameters\n","1. in_size : Input dimension(channels number) \n","2. out_size : Output dimension(channels number)\n","3. normalize : If it is true add Batch Normalization layer, otherwise skip this layer\n","4. dropout : probability for dropping a unit"]},{"cell_type":"code","metadata":{"id":"hg75UeuT8Sxp","executionInfo":{"status":"ok","timestamp":1643789696244,"user_tz":-180,"elapsed":248,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class UNetDown(nn.Module):\n","    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n","        super(UNetDown, self).__init__()\n","        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n","        if normalize:\n","            layers.append(nn.BatchNorm2d(out_size, 0.8))\n","        layers.append(nn.LeakyReLU(0.2))\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tfk-wG3O8c-V","executionInfo":{"status":"ok","timestamp":1643789697303,"user_tz":-180,"elapsed":2,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class UNetUp(nn.Module):\n","    def __init__(self, in_size, out_size):\n","        super(UNetUp, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_size, 0.8),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x, skip_input):\n","        x = self.model(x)\n","        x = torch.cat((x, skip_input), 1)\n","        return x"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Unet's analog"],"metadata":{"id":"dDZgmBXtIKe1"}},{"cell_type":"code","metadata":{"id":"U844EY5q9ErM","executionInfo":{"status":"ok","timestamp":1643789698176,"user_tz":-180,"elapsed":3,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class Generator(nn.Module):\n","    def __init__(self, latent_dim, img_shape):\n","        super(Generator, self).__init__()\n","        channels, self.h, self.w = img_shape\n","\n","        self.fc = nn.Linear(latent_dim, self.h * self.w)\n","\n","        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n","        self.down2 = UNetDown(64, 128)\n","        self.down3 = UNetDown(128, 256)\n","        self.down4 = UNetDown(256, 512)\n","        self.down5 = UNetDown(512, 512)\n","        self.down6 = UNetDown(512, 512)\n","        self.down7 = UNetDown(512, 512, normalize=False)\n","        self.up1 = UNetUp(512, 512)\n","        self.up2 = UNetUp(1024, 512)\n","        self.up3 = UNetUp(1024, 512)\n","        self.up4 = UNetUp(1024, 256)\n","        self.up5 = UNetUp(512, 128)\n","        self.up6 = UNetUp(256, 64)\n","\n","        self.final = nn.Sequential(\n","            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n","        )\n","\n","    def forward(self, x, z):\n","        # Propogate noise through fc layer and reshape to img shape\n","        #x:(N,3,128,128) z:(N,8)\n","        z = self.fc(z).view(z.size(0), 1, self.h, self.w)#z:(N,1,128,128)\n","        \n","        #concating (x and z): (N,4,128,128)\n","        d1 = self.down1(torch.cat((x, z), 1)) #d1:(N,64,64,64)\n","        d2 = self.down2(d1)         #d2:(N,128,32,32)\n","        d3 = self.down3(d2)         #d3:(N,256,16,16)\n","        d4 = self.down4(d3)         #d4:(N,512,8,8)\n","        d5 = self.down5(d4)         #d5:(N,512,4,4)\n","        d6 = self.down6(d5)         #d6:(N,512,2,2)\n","        d7 = self.down7(d6)         #d7:(N,512,1,1)\n","        u1 = self.up1(d7, d6)       #u1:(N,1024,2,2)\n","        u2 = self.up2(u1, d5)       #u2:(N,1024,4,4)\n","        u3 = self.up3(u2, d4)       #u3:(N,1024,8,8)\n","        u4 = self.up4(u3, d3)       #u4:(N,512,16,16)\n","        u5 = self.up5(u4, d2)       #u5:(N,256,32,32)\n","        u6 = self.up6(u5, d1)       #u6:(N,128,64,64)\n","\n","        return self.final(u6)       #final:(N,3,128,128)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RW6cUclMIv-H"},"source":["MultiDiscriminator uses three discriminators for object sizes 32, 64, 128"]},{"cell_type":"code","metadata":{"id":"tc4eC4jKDXXI","executionInfo":{"status":"ok","timestamp":1643789700096,"user_tz":-180,"elapsed":5,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class MultiDiscriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(MultiDiscriminator, self).__init__()\n","\n","        def discriminator_block(in_filters, out_filters, normalize=True):\n","            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n","            if normalize:\n","                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n","            layers.append(nn.LeakyReLU(0.2))\n","            return layers\n","\n","        channels, _, _ = input_shape\n","        # Extracts discriminator models\n","        self.models = nn.ModuleList()\n","        for i in range(3):\n","            self.models.add_module(\n","                \"disc_%d\" % i,\n","                nn.Sequential(\n","                    *discriminator_block(channels, 64, normalize=False),\n","                    *discriminator_block(64, 128),\n","                    *discriminator_block(128, 256),\n","                    *discriminator_block(256, 512),\n","                    nn.Conv2d(512, 1, 3, padding=1)\n","                ),\n","            )\n","\n","        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n","\n","    def compute_loss(self, x, gt):\n","        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n","        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n","        return loss\n","\n","    def forward(self, x):\n","        outputs = []\n","        for m in self.models:\n","            outputs.append(m(x))\n","            x = self.downsample(x)\n","        return outputs"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6A7H5Pm_Ioc6"},"source":["Encoder"]},{"cell_type":"code","metadata":{"id":"dglkJAJ0EQzx","executionInfo":{"status":"ok","timestamp":1643789701368,"user_tz":-180,"elapsed":1,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["class Encoder(nn.Module):\n","    def __init__(self, latent_dim, input_shape):\n","        super(Encoder, self).__init__()\n","        resnet18_model = resnet18(pretrained=False)\n","        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n","        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n","        # Output is mu and log(var) for reparameterization trick used in VAEs\n","        self.fc_mu = nn.Linear(256, latent_dim)\n","        self.fc_logvar = nn.Linear(256, latent_dim)\n","\n","    def forward(self, img):\n","        #img : (N, 3, 128, 128)\n","        out = self.feature_extractor(img)  # out : (N, 256, 8, 8)\n","        out = self.pooling(out)            # out : (N, 256, 1, 1)\n","        out = out.view(out.size(0), -1)    # out : (N, 256)\n","        mu = self.fc_mu(out)               # mu : (N, latent_dim)\n","        logvar = self.fc_logvar(out)       # logvar : (N, latent_dim)\n","        return mu, logvar"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wOSA6qQIqiD","executionInfo":{"status":"ok","timestamp":1643789703679,"user_tz":-180,"elapsed":282,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["def reparameterization(mu, logvar):\n","    std = torch.exp(logvar / 2)\n","    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), latent_dim))))\n","    z = sampled_z * std + mu\n","    return z\n","    # z : (N, latent_dim)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"qeUBnh7uKf_M","executionInfo":{"status":"ok","timestamp":1643789712451,"user_tz":-180,"elapsed":1972,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["# Archive with training files\n","!unzip -q /content/drive/MyDrive/cv_project/cv_project_s2p.zip"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWB9PZ_gJzlN","executionInfo":{"status":"ok","timestamp":1643798224093,"user_tz":-180,"elapsed":1152,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["epoch = 0                      #epoch to start training from\n","n_epochs = 150                 #number of epochs of training\n","dataset_name = \"sketch2face\"   #name of the dataset\n","batch_size = 16                 #size of the batches\n","lr = 0.0005                    #adam: learning rate\n","b1 = 0.6                       #adam: decay of first order momentum of gradient\n","b2 = 0.999                     #adam: decay of second order momentum of gradient\n","n_cpu = 8                      #number of cpu threads to use during batch generation\n","img_height = 128               #size of image height\n","img_width = 128                #size of image width\n","channels = 3                   #number of image channels\n","latent_dim = 8                 #number of latent codes\n","lambda_pixel = 10              #pixelwise loss weight\n","lambda_latent = 0.6            #latent loss weight\n","lambda_kl = 0.02               #kullback-leibler loss weight\n","mae_loss = torch.nn.L1Loss()   #Mean Absolute error loss\n","\n","\n","input_shape = (channels, img_height, img_width)       #shape of input image (tuple)\n","\n","cuda = True if torch.cuda.is_available() else False   #availability of GPU\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","\n","\n","generator = Generator(latent_dim, input_shape)    #Initialize generator\n","encoder = Encoder(latent_dim, input_shape)        #Initialize encoder\n","D_VAE = MultiDiscriminator(input_shape)           #initialize discriminators\n","D_LR = MultiDiscriminator(input_shape)\n","\n","# Uncomment for further learning\n","\n","# generator.load_state_dict(torch.load('/content/generator_100.pth'))\n","# encoder.load_state_dict(torch.load('/content/encoder_100.pth'))\n","# D_VAE.load_state_dict(torch.load('/content/images/D_VAE_100.pth'))\n","# D_LR.load_state_dict(torch.load('/content/images/D_LR_100.pth'))\n","\n","if cuda:\n","    generator = generator.cuda()\n","    encoder.cuda()\n","    D_VAE = D_VAE.cuda()\n","    D_LR = D_LR.cuda()\n","    mae_loss.cuda()\n","\n","# On initial use uncomment\n","\n","    # Initialize weights\n","    # generator.apply(weights_init_normal)\n","    # D_VAE.apply(weights_init_normal)\n","    # D_LR.apply(weights_init_normal)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_0eOMSKKJ71","executionInfo":{"status":"ok","timestamp":1643791839848,"user_tz":-180,"elapsed":275,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOoRAkuCmKU3","executionInfo":{"status":"ok","timestamp":1643789975931,"user_tz":-180,"elapsed":7421,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["def get_concat_h(im2, im1):\n","    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n","    dst.paste(im1, (0, 0))\n","    dst.paste(im2, (im1.width, 0))\n","    return dst\n","\n","\n","# defining the size of image \n","SIZE = 128\n","\n","image_path = '/content/cv_project_s2p/train/photo'\n","sketch_path = '/content/cv_project_s2p/train/sketch'\n","\n","image_file = os.listdir(image_path)\n","\n","\n","for i in range(len(image_file)):\n","    im1 = Image.open('/content/cv_project_s2p/train/photo/' + image_file[i])\n","    im2 = Image.open('/content/cv_project_s2p/train/sketch/' + image_file[i])\n","    im1 = im1.resize((SIZE, SIZE))\n","    im2 = im2.resize((SIZE, SIZE))\n","    get_concat_h(im1, im2).save('/content/cv_project_s2p/train1/' + str(i) + '.jpg')\n","\n","\n","\n","image_path = '/content/cv_project_s2p/val/photo'\n","sketch_path = '/content/cv_project_s2p/val/sketch'\n","\n","image_file = os.listdir(image_path)\n","\n","\n","for i in range(len(image_file)):\n","    im1 = Image.open('/content/cv_project_s2p/val/photo/' + image_file[i])\n","    im2 = Image.open('/content/cv_project_s2p/val/sketch/' + image_file[i])\n","    im1 = im1.resize((SIZE, SIZE))\n","    im2 = im2.resize((SIZE, SIZE))\n","    get_concat_h(im1, im2).save('/content/cv_project_s2p/val1/' + str(i) + '.jpg')\n","\n","\n","\n","image_path = '/content/cv_project_s2p/test/photo'\n","sketch_path = '/content/cv_project_s2p/test/sketch'\n","\n","image_file = os.listdir(image_path)\n","\n","for i in range(len(image_file)):\n","    im1 = Image.open('/content/cv_project_s2p/test/photo/' + image_file[i])\n","    im2 = Image.open('/content/cv_project_s2p/test/skecth/' + image_file[i])\n","    im1 = im1.resize((SIZE, SIZE))\n","    im2 = im2.resize((SIZE, SIZE))\n","    get_concat_h(im2, im1).save('/content/cv_project_s2p/test1/' + str(i) + '.jpg')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8_A24BWKSKE","executionInfo":{"status":"ok","timestamp":1643798238167,"user_tz":-180,"elapsed":265,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["optimizer_E = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=lr, betas=(b1, b2))"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZBMTA3Tk-MJ","executionInfo":{"status":"ok","timestamp":1643790011909,"user_tz":-180,"elapsed":263,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}},"outputId":"d2e53a65-8271-4a6a-9047-da55b58f5591"},"source":["dataloader = DataLoader(\n","    ImageDataset(\"/content/cv_project_s2p\", input_shape),\n","    batch_size=16,\n","    shuffle=True,\n","    num_workers=n_cpu,\n",")\n","val_dataloader = DataLoader(\n","    ImageDataset(\"/content/cv_project_s2p\", input_shape, mode='val1'),\n","    batch_size=5,\n","    shuffle=False,\n","    num_workers=1,\n",")\n","\n","test_dataloader = DataLoader(\n","    ImageDataset(\"/content/cv_project_s2p\", input_shape, mode='test1'),\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=1,\n",")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"id":"_2Nrd5PISIQI","executionInfo":{"status":"ok","timestamp":1643790021486,"user_tz":-180,"elapsed":4,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["os.makedirs(\"images/sketch2face/img_val\", exist_ok=True)\n","os.makedirs(\"images/sketch2face/img_test\", exist_ok=True)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"CndHXuzWK8AA","executionInfo":{"status":"ok","timestamp":1643790024075,"user_tz":-180,"elapsed":2,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["def sample_images(epoch_done, path=\"/content/images/sketch2face/\", mode='img_val'):\n","\n","    generator.eval()\n","    if mode == 'img_val':\n","        imgs = next(iter(val_dataloader))\n","    else:\n","        imgs = next(iter(test_dataloader))\n","    img_samples = None\n","    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n","\n","        # Repeat input image by number of desired columns\n","        real_A = img_A.view(1, *img_A.shape).repeat(latent_dim, 1, 1, 1)\n","        real_A = Variable(real_A.type(Tensor))\n","\n","        # Sample latent representations\n","        sampled_z = Variable(Tensor(np.random.normal(0, 1, (latent_dim, latent_dim))))\n","        # Generate samples\n","        fake_B = generator(real_A, sampled_z)\n","        # Concatenate samples horisontally\n","        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n","        img_sample = torch.cat((img_A, fake_B), -1)\n","        img_sample = img_sample.view(1, *img_sample.shape)\n","        # Concatenate with previous samples vertically\n","        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n","    save_image(img_samples, path + mode + \"/images_\"+ str(epoch_done) + \".png\", nrow=8, normalize=True)\n","    \n","    generator.train()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"afmXEKMsJ6Od","executionInfo":{"status":"ok","timestamp":1643790026175,"user_tz":-180,"elapsed":4,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}}},"source":["os.makedirs(\"images/sketch2face/checkpoints\", exist_ok=True)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"2ijZxvSsNme-"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kJv6oDaR2T2","executionInfo":{"status":"ok","timestamp":1643799835945,"user_tz":-180,"elapsed":1590210,"user":{"displayName":"Павел Богословский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06344329247855720367"}},"outputId":"a29315df-835d-4565-d5d0-8e12dc406a59"},"source":["# Adversarial loss\n","valid = 1\n","fake = 0\n","\n","prev_time = time.time()\n","for epoch in range(epoch, n_epochs):\n","    for i, batch in enumerate(dataloader):\n","\n","        # Set model input\n","        real_A = Variable(batch[\"A\"].type(Tensor))\n","        real_B = Variable(batch[\"B\"].type(Tensor))\n","\n","        # -------------------------------\n","        #  Train Generator and Encoder\n","        # -------------------------------\n","\n","        optimizer_E.zero_grad()\n","        optimizer_G.zero_grad()\n","\n","        # ----------\n","        # cVAE-GAN\n","        # ----------\n","\n","        # Produce output using encoding of B (cVAE-GAN)\n","        mu, logvar = encoder(real_B)\n","        encoded_z = reparameterization(mu, logvar)\n","        fake_B = generator(real_A, encoded_z)\n","\n","        # Pixelwise loss of translated image by VAE\n","        loss_pixel = mae_loss(fake_B, real_B)\n","        # Kullback-Leibler divergence of encoded B\n","        loss_kl = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - logvar - 1)\n","        # Adversarial loss\n","        loss_VAE_GAN = D_VAE.compute_loss(fake_B, valid)\n","\n","        # ---------\n","        # cLR-GAN\n","        # ---------\n","\n","        # Produce output using sampled z (cLR-GAN)\n","        sampled_z = Variable(Tensor(np.random.normal(0, 1, (real_A.size(0), latent_dim))))\n","        _fake_B = generator(real_A, sampled_z)\n","        # cLR Loss: Adversarial loss\n","        loss_LR_GAN = D_LR.compute_loss(_fake_B, valid)\n","\n","        # ----------------------------------\n","        # Total Loss (Generator + Encoder)\n","        # ----------------------------------\n","\n","        loss_GE = loss_VAE_GAN + loss_LR_GAN + lambda_pixel * loss_pixel + lambda_kl * loss_kl\n","\n","        loss_GE.backward(retain_graph=True)\n","        optimizer_E.step()\n","\n","        # ---------------------\n","        # Generator Only Loss\n","        # ---------------------\n","\n","        # Latent L1 loss\n","        _mu, _ = encoder(_fake_B)\n","        loss_latent = lambda_latent * mae_loss(_mu, sampled_z)\n","\n","        loss_latent.backward()\n","        optimizer_G.step()\n","\n","        # ----------------------------------\n","        #  Train Discriminator (cVAE-GAN)\n","        # ----------------------------------\n","\n","        optimizer_D_VAE.zero_grad()\n","\n","        loss_D_VAE = D_VAE.compute_loss(real_B, valid) + D_VAE.compute_loss(fake_B.detach(), fake)\n","\n","        loss_D_VAE.backward()\n","        optimizer_D_VAE.step()\n","\n","        # ---------------------------------\n","        #  Train Discriminator (cLR-GAN)\n","        # ---------------------------------\n","\n","        optimizer_D_LR.zero_grad()\n","\n","        loss_D_LR = D_LR.compute_loss(real_B, valid) + D_LR.compute_loss(_fake_B.detach(), fake)\n","\n","        loss_D_LR.backward()\n","        optimizer_D_LR.step()\n","\n","        # --------------\n","        #  Log Progress\n","        # --------------\n","\n","        # Determine approximate time left\n","        batches_done = epoch * len(dataloader) + i\n","        batches_left = n_epochs * len(dataloader) - batches_done\n","        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n","        prev_time = time.time()\n","\n","        # Print log\n","        sys.stdout.write(\n","            \"\\r[Epoch %d/%d] [Batch %d/%d] [D VAE_loss: %f, LR_loss: %f] [G loss: %f, pixel: %f, kl: %f, latent: %f] ETA: %s\"\n","            % (\n","                epoch,\n","                n_epochs,\n","                i,\n","                len(dataloader),\n","                loss_D_VAE.item(),\n","                loss_D_LR.item(),\n","                loss_GE.item(),\n","                loss_pixel.item(),\n","                loss_kl.item(),\n","                loss_latent.item(),\n","                time_left,\n","            )\n","        )\n","        \n","    if epoch % 4 == 0:\n","        sample_images(epoch + 1, mode='img_test')\n","        sample_images(epoch + 1, mode='img_val')\n","        torch.save(generator.state_dict(), \"/content/images/sketch2face/checkpoints/generator_\"+ str(epoch + 1) + \".pth\")\n","        torch.save(encoder.state_dict(), \"/content/images/sketch2face/checkpoints/encoder_\"+ str(epoch + 1) + \".pth\")\n","        torch.save(D_VAE.state_dict(), \"/content/images/sketch2face/checkpoints/D_VAE_\"+ str(epoch + 1) + \".pth\")\n","        torch.save(D_LR.state_dict(), \"/content/images/sketch2face/checkpoints/D_LR_\"+ str(epoch + 1) + \".pth\")"]}]}
